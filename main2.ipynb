{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14401945,"sourceType":"datasetVersion","datasetId":9198103}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4ec4c78f-05d6-413c-8b8a-2639050b8e25","cell_type":"code","source":"# Clear the Kaggle working directory\nimport os\nimport shutil\n\n# The directory to clear\nfolder = '/kaggle/working/'\n\n# Loop through everything in the folder and delete it\nfor filename in os.listdir(folder):\n    file_path = os.path.join(folder, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n    except Exception as e:\n        print(f'Failed to delete {file_path}. Reason: {e}')\n\nprint(\"✅ /kaggle/working/ directory has been cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:09:15.854522Z","iopub.execute_input":"2026-01-05T16:09:15.854787Z","iopub.status.idle":"2026-01-05T16:09:15.861161Z","shell.execute_reply.started":"2026-01-05T16:09:15.854766Z","shell.execute_reply":"2026-01-05T16:09:15.860419Z"}},"outputs":[{"name":"stdout","text":"✅ /kaggle/working/ directory has been cleared.\n","output_type":"stream"}],"execution_count":13},{"id":"c64d0fcc-d5ad-4401-a707-6f21c42f9084","cell_type":"markdown","source":"## VAR+GRU","metadata":{}},{"id":"5ef2d3f5-44fe-4084-9066-01b36fb19230","cell_type":"code","source":"\"\"\"\nVAR-GRU Hybrid (VRT Style) - KAGGLE VERSION\n-------------------------------------------\nArchitecture:\n1. Linear Stream: VAR (Vector Autoregression) for baseline trend.\n2. Non-Linear Stream: GRU (Gated Recurrent Unit) for residual correction.\n3. Fusion: Forecast = VAR_Baseline + GRU_Correction.\n\"\"\"\n\nimport os\nimport glob\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.api import VAR\n\n# ==========================================\n# 0. Kaggle Configuration & Setup\n# ==========================================\n# Suppress TF warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# GPU Memory Growth\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"[GPU] Acceleration Enabled\")\n    except RuntimeError as e:\n        print(e)\n\nCONFIG = {\n    # Path will be auto-detected\n    'dataset_path': None, \n    \n    # Data params\n    'train_split': 0.7,\n    'window_size': 24,       \n    'forecast_horizon': 1,   \n    \n    # VAR params\n    'var_max_lags': 15,\n    \n    # GRU Hyperparameters\n    'gru_units': 64,         \n    'dense_units': 32,\n    'dropout': 0.1,\n    'learning_rate': 1e-3,\n    'batch_size': 32,\n    'epochs': 100\n}\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ==========================================\n# 1. Advanced Data Loading & Engineering\n# ==========================================\ndef find_dataset_file():\n    \"\"\"Auto-detects the dataset file in Kaggle input directory\"\"\"\n    search_path = '/kaggle/input'\n    print(f\"[SEARCH] Looking for dataset in {search_path}...\")\n    \n    for root, dirs, files in os.walk(search_path):\n        for file in files:\n            if file.endswith(\".csv\") or file.endswith(\".txt\") or file.endswith(\".dat\"):\n                full_path = os.path.join(root, file)\n                print(f\"[FOUND] Dataset located: {full_path}\")\n                return full_path\n    \n    raise FileNotFoundError(\"Could not find a dataset file in /kaggle/input\")\n\ndef load_and_engineer_features(filepath):\n    print(f\"\\n[IO] Loading raw data: {filepath}\")\n    \n    try:\n        # Try reading as standard CSV first\n        try:\n            df = pd.read_csv(filepath)\n            if df.shape[1] < 2 or '::' in str(df.iloc[0,0]):\n                raise ValueError(\"Likely raw format\")\n        except:\n            print(\"  > Detected raw format (parsing '::')...\")\n            df = pd.read_csv(filepath, sep='\\t', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n            \n            if df.shape[1] == 1:\n                 df = pd.read_csv(filepath, sep=',', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n\n            split_data = df['slice_timestamp'].str.split('::', expand=True)\n            df['slice_type'] = split_data[0]\n            df['timestamp'] = pd.to_numeric(split_data[1])\n            df['bytes'] = pd.to_numeric(df['bytes'], errors='coerce')\n            df['packets'] = pd.to_numeric(df['packets'], errors='coerce')\n\n        if 'slice_label' in df.columns: df.rename(columns={'slice_label': 'slice_type'}, inplace=True)\n        \n        processed_slices = {}\n        \n        group_col = 'slice_type' if 'slice_type' in df.columns else 'slice_id'\n        if group_col not in df.columns:\n            df['slice_type'] = 'Default_Slice'\n            group_col = 'slice_type'\n\n        for slice_id in df[group_col].unique():\n            print(f\"  > Processing Slice: {slice_id}\")\n            slice_df = df[df[group_col] == slice_id].sort_values('timestamp').copy()\n            \n            if len(slice_df) < 500: continue \n\n            # 1. Map Columns\n            if 'sum_bytes' in slice_df.columns: slice_df['throughput'] = slice_df['sum_bytes']\n            elif 'bytes' in slice_df.columns: slice_df['throughput'] = slice_df['bytes']\n            \n            if 'sum_packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['sum_packets']\n            elif 'packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['packets']\n            \n            # 2. Engineering: Velocity (Diff) & Volatility (Std)\n            slice_df['throughput_diff'] = slice_df['throughput'].diff()\n            slice_df['packet_diff'] = slice_df['packet_rate'].diff()\n            slice_df['volatility'] = slice_df['throughput'].rolling(5).std().fillna(0)\n            \n            # 3. Select Features\n            cols = ['throughput', 'packet_rate', 'throughput_diff', 'packet_diff', 'volatility']\n            final_df = slice_df[cols].dropna()\n            \n            # 4. Clip Outliers\n            p99 = final_df.quantile(0.99)\n            final_df = final_df.clip(upper=p99, axis=1)\n            \n            processed_slices[slice_id] = final_df\n            \n        return processed_slices\n\n    except Exception as e:\n        print(f\"[ERROR] Loading failed: {e}\")\n        return {}\n\n# ==========================================\n# 2. Strict Leakage-Free VAR Baseline\n# ==========================================\ndef get_var_residuals(train_df, test_df, maxlags):\n    # Noise injection for constant columns\n    for col in train_df.columns:\n        if train_df[col].nunique() <= 1:\n            train_df[col] += np.random.normal(0, 1e-6, size=len(train_df))\n\n    # 1. Fit VAR on Train\n    model = VAR(train_df)\n    try:\n        lag_order_res = model.select_order(maxlags=maxlags)\n        lag_order = lag_order_res.aic\n        if lag_order < 1: lag_order = 10\n    except:\n        lag_order = 10\n        \n    var_results = model.fit(lag_order)\n    print(f\"  [VAR] Fitted with Lag Order: {lag_order}\")\n    \n    # 2. Train Residuals\n    train_pred = var_results.fittedvalues\n    train_actual = train_df.iloc[lag_order:]\n    train_residuals = train_actual - train_pred\n\n    # 3. Test Baseline (Rolling Forecast)\n    coefs = var_results.coefs\n    intercept = var_results.intercept\n    \n    history = pd.concat([train_df.iloc[-lag_order:], test_df])\n    history_values = history.values\n    \n    test_preds = []\n    \n    for i in range(lag_order, len(history_values)):\n        window = history_values[i-lag_order : i]\n        window_reversed = window[::-1]\n        \n        pred = intercept.copy()\n        for l in range(lag_order):\n            pred += np.dot(coefs[l], window_reversed[l])\n            \n        test_preds.append(pred)\n        \n    test_pred_df = pd.DataFrame(test_preds, index=test_df.index, columns=test_df.columns)\n    \n    # 4. Test Residuals\n    test_residuals = test_df - test_pred_df\n    \n    return train_residuals, test_residuals, test_pred_df\n\n# ==========================================\n# 3. GRU Model Architecture (VRT Style)\n# ==========================================\ndef build_gru_model(input_shape, output_dim, config):\n    inputs = layers.Input(shape=input_shape)\n    \n    # --- GRU Layer ---\n    # Using return_sequences=True followed by Pooling is often more stable\n    x = layers.GRU(config['gru_units'], return_sequences=True, activation='tanh', name=\"GRU_Layer\")(inputs)\n    x = layers.Dropout(config['dropout'])(x)\n    \n    # --- Decoding ---\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(config['dense_units'], activation='relu')(x)\n    \n    # Final Linear Projection\n    outputs = layers.Dense(output_dim, activation='linear', name=\"Residual_Output\")(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name=\"VAR_GRU_Hybrid\")\n    \n    optimizer = optimizers.Adam(learning_rate=config['learning_rate'])\n    # Using Huber loss for robustness (Standard VRT Feature)\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n    \n    return model\n\n# ==========================================\n# 4. Helpers\n# ==========================================\ndef create_windows(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i : i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# ==========================================\n# 5. Main Execution Pipeline\n# ==========================================\ndef train_evaluate_slice(slice_name, df):\n    print(f\"\\n{'='*40}\\n PROCESSING: {slice_name}\\n{'='*40}\")\n    \n    # A. Split Data\n    split_idx = int(len(df) * CONFIG['train_split'])\n    train_raw = df.iloc[:split_idx]\n    test_raw = df.iloc[split_idx:]\n    \n    if len(test_raw) < CONFIG['window_size'] + 50:\n        print(\"  [SKIP] Not enough data for testing.\")\n        return None\n\n    # B. The Linear Baseline (VAR)\n    print(\"  [1/4] Calculating VAR Residuals...\")\n    try:\n        train_res, test_res, test_var_baseline = get_var_residuals(\n            train_raw, test_raw, CONFIG['var_max_lags']\n        )\n    except Exception as e:\n        print(f\"  [VAR FAILED] {e}. Skipping slice.\")\n        return None\n    \n    # C. Preprocessing for Deep Learning\n    print(\"  [2/4] Scaling & Windowing...\")\n    scaler = StandardScaler()\n    train_res_scaled = scaler.fit_transform(train_res)\n    test_res_scaled = scaler.transform(test_res)\n    \n    X_train, y_train = create_windows(train_res_scaled, CONFIG['window_size'])\n    X_test, y_test = create_windows(test_res_scaled, CONFIG['window_size'])\n    \n    if len(X_train) == 0: return None\n\n    # D. Train GRU Model\n    print(f\"  [3/4] Training GRU ({len(X_train)} samples)...\")\n    model = build_gru_model(\n        input_shape=(CONFIG['window_size'], X_train.shape[2]),\n        output_dim=y_train.shape[1],\n        config=CONFIG\n    )\n    \n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=CONFIG['epochs'],\n        batch_size=CONFIG['batch_size'],\n        callbacks=[early_stop, reduce_lr],\n        verbose=0\n    )\n    print(f\"      > Final Val Loss: {history.history['val_loss'][-1]:.5f}\")\n    \n    # E. Final Prediction & Fusion\n    print(\"  [4/4] Forecasting & Fusion...\")\n    \n    # 1. Predict Scaled Residuals\n    pred_res_scaled = model.predict(X_test, verbose=0)\n    \n    # 2. Inverse Scale -> Real Residuals\n    pred_res = scaler.inverse_transform(pred_res_scaled)\n    \n    # 3. Align Baseline\n    baseline_aligned = test_var_baseline.iloc[CONFIG['window_size']:].values\n    y_true_aligned = test_raw.iloc[CONFIG['window_size']:].values\n    \n    # Truncate to matching lengths\n    min_len = min(len(baseline_aligned), len(pred_res))\n    baseline_aligned = baseline_aligned[:min_len]\n    y_true_aligned = y_true_aligned[:min_len]\n    pred_res = pred_res[:min_len]\n    \n    # 4. FUSION\n    final_forecast = baseline_aligned + pred_res\n    final_forecast = np.maximum(final_forecast, 0)\n    \n    # Metrics\n    mse = np.mean((y_true_aligned - final_forecast)**2, axis=0)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true_aligned - final_forecast), axis=0)\n    \n    # Plotting\n    plt.figure(figsize=(12, 4))\n    idx = 0 # Throughput\n    plt.plot(y_true_aligned[:, idx], label='Actual', color='black', alpha=0.6)\n    plt.plot(baseline_aligned[:, idx], label='VAR Only', color='orange', linestyle='--', alpha=0.7)\n    plt.plot(final_forecast[:, idx], label='VAR+GRU (Final)', color='green', linewidth=1.5)\n    plt.title(f\"Slice: {slice_name} | RMSE: {rmse[idx]:.2f}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return {\n        'slice': slice_name,\n        'rmse_throughput': rmse[0],\n        'mae_throughput': mae[0]\n    }\n\ndef main():\n    try:\n        # Auto-detect file\n        filepath = find_dataset_file()\n        CONFIG['dataset_path'] = filepath\n        \n        slices_data = load_and_engineer_features(filepath)\n        \n        results = []\n        for name, data in slices_data.items():\n            res = train_evaluate_slice(name, data)\n            if res: results.append(res)\n            \n        if results:\n            res_df = pd.DataFrame(results)\n            print(\"\\nFinal Results Summary (VAR + GRU):\")\n            print(res_df)\n    except Exception as e:\n        print(f\"Critical Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:43:39.420171Z","iopub.execute_input":"2026-01-05T16:43:39.420509Z","iopub.status.idle":"2026-01-05T16:45:26.205275Z","shell.execute_reply.started":"2026-01-05T16:43:39.420476Z","shell.execute_reply":"2026-01-05T16:45:26.204677Z"}},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n[SEARCH] Looking for dataset in /kaggle/input...\n[FOUND] Dataset located: /kaggle/input/feature-extracted/part-00000.csv\n\n[IO] Loading raw data: /kaggle/input/feature-extracted/part-00000.csv\n  > Processing Slice: MMTC\n  > Processing Slice: Naver\n  > Processing Slice: Youtube\n\n========================================\n PROCESSING: MMTC\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 10\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU (1379 samples)...\n      > Final Val Loss: 0.15343\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Naver\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU (2765 samples)...\n      > Final Val Loss: 0.19457\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Youtube\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU (10483 samples)...\n      > Final Val Loss: 0.47434\n  [4/4] Forecasting & Fusion...\n\nFinal Results Summary (VAR + GRU):\n     slice  rmse_throughput  mae_throughput\n0     MMTC      5983.788178     1085.507324\n1    Naver    899320.757439   654366.283389\n2  Youtube    964825.491253   779503.070344\n","output_type":"stream"}],"execution_count":21},{"id":"76688ca1-b4a0-478d-8d6a-8f631d427330","cell_type":"markdown","source":"## VAR+GRU+TFT","metadata":{}},{"id":"1b8d1ca7-2df9-4324-b7e7-cd59865b6508","cell_type":"code","source":"\"\"\"\nHybrid VAR-GRU-TFT (Temporal Fusion Transformer) - KAGGLE VERSION\n-----------------------------------------------------------------\nArchitecture:\n1. Linear Stream: VAR (Vector Autoregression) for baseline trend.\n2. Non-Linear Stream: GRU + Multi-Head Attention for residual correction.\n3. Fusion: Forecast = VAR_Baseline + (GRU+TFT)_Correction.\n\"\"\"\n\nimport os\nimport glob\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.layers import Input, GRU, Dense, Dropout, MultiHeadAttention, LayerNormalization, Add, GlobalAveragePooling1D\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.api import VAR\n\n# ==========================================\n# 0. Kaggle Configuration & Setup\n# ==========================================\n# Suppress TF warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# GPU Memory Growth\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"[GPU] Acceleration Enabled\")\n    except RuntimeError as e:\n        print(e)\n\nCONFIG = {\n    # Path will be auto-detected\n    'dataset_path': None, \n    \n    # Data params\n    'train_split': 0.7,\n    'window_size': 24,       \n    'forecast_horizon': 1,   \n    \n    # VAR params\n    'var_max_lags': 15,\n    \n    # Model Hyperparameters (GRU + TFT)\n    'gru_units': 64,         # Size of GRU hidden state\n    'head_size': 64,         # Key dimension for Attention\n    'num_heads': 4,          # Number of Attention Heads\n    'dropout': 0.15,\n    'learning_rate': 1e-3,\n    'batch_size': 32,\n    'epochs': 150\n}\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ==========================================\n# 1. Advanced Data Loading & Engineering\n# ==========================================\ndef find_dataset_file():\n    \"\"\"Auto-detects the dataset file in Kaggle input directory\"\"\"\n    search_path = '/kaggle/input'\n    print(f\"[SEARCH] Looking for dataset in {search_path}...\")\n    \n    for root, dirs, files in os.walk(search_path):\n        for file in files:\n            if file.endswith(\".csv\") or file.endswith(\".txt\") or file.endswith(\".dat\"):\n                full_path = os.path.join(root, file)\n                print(f\"[FOUND] Dataset located: {full_path}\")\n                return full_path\n    \n    raise FileNotFoundError(\"Could not find a dataset file in /kaggle/input\")\n\ndef load_and_engineer_features(filepath):\n    print(f\"\\n[IO] Loading raw data: {filepath}\")\n    \n    try:\n        # Try reading as standard CSV first\n        try:\n            df = pd.read_csv(filepath)\n            if df.shape[1] < 2 or '::' in str(df.iloc[0,0]):\n                raise ValueError(\"Likely raw format\")\n        except:\n            print(\"  > Detected raw format (parsing '::')...\")\n            df = pd.read_csv(filepath, sep='\\t', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n            \n            if df.shape[1] == 1:\n                 df = pd.read_csv(filepath, sep=',', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n\n            split_data = df['slice_timestamp'].str.split('::', expand=True)\n            df['slice_type'] = split_data[0]\n            df['timestamp'] = pd.to_numeric(split_data[1])\n            df['bytes'] = pd.to_numeric(df['bytes'], errors='coerce')\n            df['packets'] = pd.to_numeric(df['packets'], errors='coerce')\n\n        if 'slice_label' in df.columns: df.rename(columns={'slice_label': 'slice_type'}, inplace=True)\n        \n        processed_slices = {}\n        \n        group_col = 'slice_type' if 'slice_type' in df.columns else 'slice_id'\n        if group_col not in df.columns:\n            df['slice_type'] = 'Default_Slice'\n            group_col = 'slice_type'\n\n        for slice_id in df[group_col].unique():\n            print(f\"  > Processing Slice: {slice_id}\")\n            slice_df = df[df[group_col] == slice_id].sort_values('timestamp').copy()\n            \n            if len(slice_df) < 500: continue \n\n            # 1. Map Columns\n            if 'sum_bytes' in slice_df.columns: slice_df['throughput'] = slice_df['sum_bytes']\n            elif 'bytes' in slice_df.columns: slice_df['throughput'] = slice_df['bytes']\n            \n            if 'sum_packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['sum_packets']\n            elif 'packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['packets']\n            \n            # 2. Engineering: Velocity (Diff) & Volatility (Std)\n            slice_df['throughput_diff'] = slice_df['throughput'].diff()\n            slice_df['packet_diff'] = slice_df['packet_rate'].diff()\n            slice_df['volatility'] = slice_df['throughput'].rolling(5).std().fillna(0)\n            \n            # 3. Select Features\n            cols = ['throughput', 'packet_rate', 'throughput_diff', 'packet_diff', 'volatility']\n            final_df = slice_df[cols].dropna()\n            \n            # 4. Clip Outliers\n            p99 = final_df.quantile(0.99)\n            final_df = final_df.clip(upper=p99, axis=1)\n            \n            processed_slices[slice_id] = final_df\n            \n        return processed_slices\n\n    except Exception as e:\n        print(f\"[ERROR] Loading failed: {e}\")\n        return {}\n\n# ==========================================\n# 2. Strict Leakage-Free VAR Baseline\n# ==========================================\ndef get_var_residuals(train_df, test_df, maxlags):\n    # Noise injection for constant columns\n    for col in train_df.columns:\n        if train_df[col].nunique() <= 1:\n            train_df[col] += np.random.normal(0, 1e-6, size=len(train_df))\n\n    # 1. Fit VAR on Train\n    model = VAR(train_df)\n    try:\n        lag_order_res = model.select_order(maxlags=maxlags)\n        lag_order = lag_order_res.aic\n        if lag_order < 1: lag_order = 10\n    except:\n        lag_order = 10\n        \n    var_results = model.fit(lag_order)\n    print(f\"  [VAR] Fitted with Lag Order: {lag_order}\")\n    \n    # 2. Train Residuals\n    train_pred = var_results.fittedvalues\n    train_actual = train_df.iloc[lag_order:]\n    train_residuals = train_actual - train_pred\n\n    # 3. Test Baseline (Rolling Forecast)\n    coefs = var_results.coefs\n    intercept = var_results.intercept\n    \n    history = pd.concat([train_df.iloc[-lag_order:], test_df])\n    history_values = history.values\n    \n    test_preds = []\n    \n    for i in range(lag_order, len(history_values)):\n        window = history_values[i-lag_order : i]\n        window_reversed = window[::-1]\n        \n        pred = intercept.copy()\n        for l in range(lag_order):\n            pred += np.dot(coefs[l], window_reversed[l])\n            \n        test_preds.append(pred)\n        \n    test_pred_df = pd.DataFrame(test_preds, index=test_df.index, columns=test_df.columns)\n    \n    # 4. Test Residuals\n    test_residuals = test_df - test_pred_df\n    \n    return train_residuals, test_residuals, test_pred_df\n\n# ==========================================\n# 3. GRU -> TFT Model Architecture\n# ==========================================\ndef build_tft_hybrid_model(input_shape, output_dim, config):\n    inputs = Input(shape=input_shape)\n    \n    # --- LAYER 1: GRU (Sequence Processing) ---\n    # return_sequences=True is critical for Attention to work\n    x = GRU(config['gru_units'], return_sequences=True, activation='tanh', name=\"GRU_Seq\")(inputs)\n    x = Dropout(config['dropout'])(x)\n    \n    # --- LAYER 2: TFT / Attention Block ---\n    # Self-Attention on the GRU output\n    attn_out = MultiHeadAttention(\n        num_heads=config['num_heads'], \n        key_dim=config['head_size'], \n        name=\"TFT_Attention\"\n    )(x, x)\n    \n    # --- LAYER 3: Residual Connection & Norm ---\n    # Combine Sequential (GRU) + Global Context (Attention)\n    x = Add(name=\"Skip_Connection\")([x, attn_out])\n    x = LayerNormalization(epsilon=1e-6, name=\"TFT_Norm\")(x)\n    \n    # --- LAYER 4: Decoding ---\n    x = GlobalAveragePooling1D()(x) # Flatten time dimension\n    x = Dropout(config['dropout'])(x)\n    \n    # Final Linear Projection\n    outputs = Dense(output_dim, activation='linear', name=\"Residual_Output\")(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name=\"VAR_GRU_TFT\")\n    \n    optimizer = optimizers.Adam(learning_rate=config['learning_rate'])\n    # Using Huber loss (VRT style) for robustness against bursts\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n    \n    return model\n\n# ==========================================\n# 4. Helpers\n# ==========================================\ndef create_windows(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i : i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# ==========================================\n# 5. Main Execution Pipeline\n# ==========================================\ndef train_evaluate_slice(slice_name, df):\n    print(f\"\\n{'='*40}\\n PROCESSING: {slice_name}\\n{'='*40}\")\n    \n    # A. Split Data\n    split_idx = int(len(df) * CONFIG['train_split'])\n    train_raw = df.iloc[:split_idx]\n    test_raw = df.iloc[split_idx:]\n    \n    if len(test_raw) < CONFIG['window_size'] + 50:\n        print(\"  [SKIP] Not enough data for testing.\")\n        return None\n\n    # B. The Linear Baseline (VAR)\n    print(\"  [1/4] Calculating VAR Residuals...\")\n    try:\n        train_res, test_res, test_var_baseline = get_var_residuals(\n            train_raw, test_raw, CONFIG['var_max_lags']\n        )\n    except Exception as e:\n        print(f\"  [VAR FAILED] {e}. Skipping slice.\")\n        return None\n    \n    # C. Preprocessing for Deep Learning\n    print(\"  [2/4] Scaling & Windowing...\")\n    scaler = StandardScaler()\n    train_res_scaled = scaler.fit_transform(train_res)\n    test_res_scaled = scaler.transform(test_res)\n    \n    X_train, y_train = create_windows(train_res_scaled, CONFIG['window_size'])\n    X_test, y_test = create_windows(test_res_scaled, CONFIG['window_size'])\n    \n    if len(X_train) == 0: return None\n\n    # D. Train GRU-TFT Model\n    print(f\"  [3/4] Training GRU-TFT ({len(X_train)} samples)...\")\n    model = build_tft_hybrid_model(\n        input_shape=(CONFIG['window_size'], X_train.shape[2]),\n        output_dim=y_train.shape[1],\n        config=CONFIG\n    )\n    \n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=CONFIG['epochs'],\n        batch_size=CONFIG['batch_size'],\n        callbacks=[early_stop, reduce_lr],\n        verbose=0\n    )\n    print(f\"      > Final Val Loss: {history.history['val_loss'][-1]:.5f}\")\n    \n    # E. Final Prediction & Fusion\n    print(\"  [4/4] Forecasting & Fusion...\")\n    \n    # 1. Predict Scaled Residuals\n    pred_res_scaled = model.predict(X_test, verbose=0)\n    \n    # 2. Inverse Scale -> Real Residuals\n    pred_res = scaler.inverse_transform(pred_res_scaled)\n    \n    # 3. Align Baseline\n    baseline_aligned = test_var_baseline.iloc[CONFIG['window_size']:].values\n    y_true_aligned = test_raw.iloc[CONFIG['window_size']:].values\n    \n    # Truncate to matching lengths\n    min_len = min(len(baseline_aligned), len(pred_res))\n    baseline_aligned = baseline_aligned[:min_len]\n    y_true_aligned = y_true_aligned[:min_len]\n    pred_res = pred_res[:min_len]\n    \n    # 4. FUSION\n    final_forecast = baseline_aligned + pred_res\n    final_forecast = np.maximum(final_forecast, 0)\n    \n    # Metrics\n    mse = np.mean((y_true_aligned - final_forecast)**2, axis=0)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true_aligned - final_forecast), axis=0)\n    \n    # Plotting\n    plt.figure(figsize=(12, 4))\n    idx = 0 # Throughput\n    plt.plot(y_true_aligned[:, idx], label='Actual', color='black', alpha=0.6)\n    plt.plot(baseline_aligned[:, idx], label='VAR Only', color='orange', linestyle='--', alpha=0.7)\n    plt.plot(final_forecast[:, idx], label='VAR+GRU+TFT (Final)', color='blue', linewidth=1.5)\n    plt.title(f\"Slice: {slice_name} | RMSE: {rmse[idx]:.2f}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return {\n        'slice': slice_name,\n        'rmse_throughput': rmse[0],\n        'mae_throughput': mae[0]\n    }\n\ndef main():\n    try:\n        # Auto-detect file\n        filepath = find_dataset_file()\n        CONFIG['dataset_path'] = filepath\n        \n        slices_data = load_and_engineer_features(filepath)\n        \n        results = []\n        for name, data in slices_data.items():\n            res = train_evaluate_slice(name, data)\n            if res: results.append(res)\n            \n        if results:\n            res_df = pd.DataFrame(results)\n            print(\"\\nFinal Results Summary (VAR + GRU + TFT):\")\n            print(res_df)\n    except Exception as e:\n        print(f\"Critical Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:40:09.468656Z","iopub.execute_input":"2026-01-05T16:40:09.469108Z","iopub.status.idle":"2026-01-05T16:43:39.418624Z","shell.execute_reply.started":"2026-01-05T16:40:09.469077Z","shell.execute_reply":"2026-01-05T16:43:39.417688Z"}},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n[SEARCH] Looking for dataset in /kaggle/input...\n[FOUND] Dataset located: /kaggle/input/feature-extracted/part-00000.csv\n\n[IO] Loading raw data: /kaggle/input/feature-extracted/part-00000.csv\n  > Processing Slice: MMTC\n  > Processing Slice: Naver\n  > Processing Slice: Youtube\n\n========================================\n PROCESSING: MMTC\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 10\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TFT (1379 samples)...\n      > Final Val Loss: 0.14843\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Naver\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TFT (2765 samples)...\n      > Final Val Loss: 0.19390\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Youtube\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TFT (10483 samples)...\n      > Final Val Loss: 0.46931\n  [4/4] Forecasting & Fusion...\n\nFinal Results Summary (VAR + GRU + TFT):\n     slice  rmse_throughput  mae_throughput\n0     MMTC      5894.143230     1154.303040\n1    Naver    916700.750547   682464.718177\n2  Youtube    956520.490395   775075.237329\n","output_type":"stream"}],"execution_count":20},{"id":"ffc8bab5-e1d3-4aca-b003-91197e4209a4","cell_type":"markdown","source":"## VAR+GRU+TCN","metadata":{}},{"id":"2ba634aa-9135-45d1-826c-9940a286376d","cell_type":"code","source":"\"\"\"\nHybrid VAR-GRU-TCN (Temporal Convolutional Network) - KAGGLE VERSION\n--------------------------------------------------------------------\nArchitecture:\n1. Linear Stream: VAR (Vector Autoregression) for baseline trend.\n2. Non-Linear Stream: GRU + Causal TCN for residual correction.\n3. Fusion: Forecast = VAR_Baseline + (GRU+TCN)_Correction.\n\"\"\"\n\nimport os\nimport glob\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.layers import Input, GRU, Dense, Dropout, Conv1D, LayerNormalization, Add, GlobalAveragePooling1D\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.api import VAR\n\n# ==========================================\n# 0. Kaggle Configuration & Setup\n# ==========================================\n# Suppress TF warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# GPU Memory Growth\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"[GPU] Acceleration Enabled\")\n    except RuntimeError as e:\n        print(e)\n\nCONFIG = {\n    # Path will be auto-detected\n    'dataset_path': None, \n    \n    # Data params\n    'train_split': 0.7,\n    'window_size': 24,       \n    'forecast_horizon': 1,   \n    \n    # VAR params\n    'var_max_lags': 15,\n    \n    # Model Hyperparameters (GRU + TCN)\n    'gru_units': 64,         \n    'tcn_filters': 64,       # Must match GRU units for Add() layer\n    'tcn_kernel': 3,         # Convolution window size\n    'dropout': 0.15,\n    'learning_rate': 1e-3,\n    'batch_size': 32,\n    'epochs': 150\n}\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ==========================================\n# 1. Advanced Data Loading & Engineering\n# ==========================================\ndef find_dataset_file():\n    \"\"\"Auto-detects the dataset file in Kaggle input directory\"\"\"\n    search_path = '/kaggle/input'\n    print(f\"[SEARCH] Looking for dataset in {search_path}...\")\n    \n    for root, dirs, files in os.walk(search_path):\n        for file in files:\n            if file.endswith(\".csv\") or file.endswith(\".txt\") or file.endswith(\".dat\"):\n                full_path = os.path.join(root, file)\n                print(f\"[FOUND] Dataset located: {full_path}\")\n                return full_path\n    \n    raise FileNotFoundError(\"Could not find a dataset file in /kaggle/input\")\n\ndef load_and_engineer_features(filepath):\n    print(f\"\\n[IO] Loading raw data: {filepath}\")\n    \n    try:\n        # Try reading as standard CSV first\n        try:\n            df = pd.read_csv(filepath)\n            if df.shape[1] < 2 or '::' in str(df.iloc[0,0]):\n                raise ValueError(\"Likely raw format\")\n        except:\n            print(\"  > Detected raw format (parsing '::')...\")\n            df = pd.read_csv(filepath, sep='\\t', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n            \n            if df.shape[1] == 1:\n                 df = pd.read_csv(filepath, sep=',', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n\n            split_data = df['slice_timestamp'].str.split('::', expand=True)\n            df['slice_type'] = split_data[0]\n            df['timestamp'] = pd.to_numeric(split_data[1])\n            df['bytes'] = pd.to_numeric(df['bytes'], errors='coerce')\n            df['packets'] = pd.to_numeric(df['packets'], errors='coerce')\n\n        if 'slice_label' in df.columns: df.rename(columns={'slice_label': 'slice_type'}, inplace=True)\n        \n        processed_slices = {}\n        \n        group_col = 'slice_type' if 'slice_type' in df.columns else 'slice_id'\n        if group_col not in df.columns:\n            df['slice_type'] = 'Default_Slice'\n            group_col = 'slice_type'\n\n        for slice_id in df[group_col].unique():\n            print(f\"  > Processing Slice: {slice_id}\")\n            slice_df = df[df[group_col] == slice_id].sort_values('timestamp').copy()\n            \n            if len(slice_df) < 500: continue \n\n            # 1. Map Columns\n            if 'sum_bytes' in slice_df.columns: slice_df['throughput'] = slice_df['sum_bytes']\n            elif 'bytes' in slice_df.columns: slice_df['throughput'] = slice_df['bytes']\n            \n            if 'sum_packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['sum_packets']\n            elif 'packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['packets']\n            \n            # 2. Engineering: Velocity (Diff) & Volatility (Std)\n            slice_df['throughput_diff'] = slice_df['throughput'].diff()\n            slice_df['packet_diff'] = slice_df['packet_rate'].diff()\n            slice_df['volatility'] = slice_df['throughput'].rolling(5).std().fillna(0)\n            \n            # 3. Select Features\n            cols = ['throughput', 'packet_rate', 'throughput_diff', 'packet_diff', 'volatility']\n            final_df = slice_df[cols].dropna()\n            \n            # 4. Clip Outliers\n            p99 = final_df.quantile(0.99)\n            final_df = final_df.clip(upper=p99, axis=1)\n            \n            processed_slices[slice_id] = final_df\n            \n        return processed_slices\n\n    except Exception as e:\n        print(f\"[ERROR] Loading failed: {e}\")\n        return {}\n\n# ==========================================\n# 2. Strict Leakage-Free VAR Baseline\n# ==========================================\ndef get_var_residuals(train_df, test_df, maxlags):\n    # Noise injection for constant columns\n    for col in train_df.columns:\n        if train_df[col].nunique() <= 1:\n            train_df[col] += np.random.normal(0, 1e-6, size=len(train_df))\n\n    # 1. Fit VAR on Train\n    model = VAR(train_df)\n    try:\n        lag_order_res = model.select_order(maxlags=maxlags)\n        lag_order = lag_order_res.aic\n        if lag_order < 1: lag_order = 10\n    except:\n        lag_order = 10\n        \n    var_results = model.fit(lag_order)\n    print(f\"  [VAR] Fitted with Lag Order: {lag_order}\")\n    \n    # 2. Train Residuals\n    train_pred = var_results.fittedvalues\n    train_actual = train_df.iloc[lag_order:]\n    train_residuals = train_actual - train_pred\n\n    # 3. Test Baseline (Rolling Forecast)\n    coefs = var_results.coefs\n    intercept = var_results.intercept\n    \n    history = pd.concat([train_df.iloc[-lag_order:], test_df])\n    history_values = history.values\n    \n    test_preds = []\n    \n    for i in range(lag_order, len(history_values)):\n        window = history_values[i-lag_order : i]\n        window_reversed = window[::-1]\n        \n        pred = intercept.copy()\n        for l in range(lag_order):\n            pred += np.dot(coefs[l], window_reversed[l])\n            \n        test_preds.append(pred)\n        \n    test_pred_df = pd.DataFrame(test_preds, index=test_df.index, columns=test_df.columns)\n    \n    # 4. Test Residuals\n    test_residuals = test_df - test_pred_df\n    \n    return train_residuals, test_residuals, test_pred_df\n\n# ==========================================\n# 3. GRU -> TCN Model Architecture\n# ==========================================\ndef build_tcn_hybrid_model(input_shape, output_dim, config):\n    inputs = Input(shape=input_shape)\n    \n    # --- LAYER 1: GRU (Sequence Processing) ---\n    # return_sequences=True allows TCN to see the full time history\n    x = GRU(config['gru_units'], return_sequences=True, activation='tanh', name=\"GRU_Seq\")(inputs)\n    x = Dropout(config['dropout'])(x)\n    \n    # --- LAYER 2: TCN (Causal Convolution) ---\n    # 1st Conv Block - Extracts local bursts\n    tcn_out = Conv1D(filters=config['tcn_filters'], \n                     kernel_size=config['tcn_kernel'], \n                     padding='causal', \n                     activation='relu', \n                     name=\"TCN_Block_1\")(x)\n    \n    # 2nd Conv Block - Refines patterns\n    tcn_out = Conv1D(filters=config['tcn_filters'], \n                     kernel_size=config['tcn_kernel'], \n                     padding='causal', \n                     activation='relu', \n                     name=\"TCN_Block_2\")(tcn_out)\n    \n    # --- LAYER 3: Residual Connection ---\n    # Combine Time Dynamics (GRU) + Local Patterns (TCN)\n    x = Add(name=\"Skip_Connection\")([x, tcn_out])\n    x = LayerNormalization(epsilon=1e-6, name=\"TCN_Norm\")(x)\n    \n    # --- LAYER 4: Decoding ---\n    x = GlobalAveragePooling1D()(x) # Flatten time dimension\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(config['dropout'])(x)\n    \n    # Final Linear Projection\n    outputs = Dense(output_dim, activation='linear', name=\"Residual_Output\")(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name=\"VAR_GRU_TCN\")\n    \n    optimizer = optimizers.Adam(learning_rate=config['learning_rate'])\n    # Huber Loss for robustness\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n    \n    return model\n\n# ==========================================\n# 4. Helpers\n# ==========================================\ndef create_windows(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i : i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# ==========================================\n# 5. Main Execution Pipeline\n# ==========================================\ndef train_evaluate_slice(slice_name, df):\n    print(f\"\\n{'='*40}\\n PROCESSING: {slice_name}\\n{'='*40}\")\n    \n    # A. Split Data\n    split_idx = int(len(df) * CONFIG['train_split'])\n    train_raw = df.iloc[:split_idx]\n    test_raw = df.iloc[split_idx:]\n    \n    if len(test_raw) < CONFIG['window_size'] + 50:\n        print(\"  [SKIP] Not enough data for testing.\")\n        return None\n\n    # B. The Linear Baseline (VAR)\n    print(\"  [1/4] Calculating VAR Residuals...\")\n    try:\n        train_res, test_res, test_var_baseline = get_var_residuals(\n            train_raw, test_raw, CONFIG['var_max_lags']\n        )\n    except Exception as e:\n        print(f\"  [VAR FAILED] {e}. Skipping slice.\")\n        return None\n    \n    # C. Preprocessing for Deep Learning\n    print(\"  [2/4] Scaling & Windowing...\")\n    scaler = StandardScaler()\n    train_res_scaled = scaler.fit_transform(train_res)\n    test_res_scaled = scaler.transform(test_res)\n    \n    X_train, y_train = create_windows(train_res_scaled, CONFIG['window_size'])\n    X_test, y_test = create_windows(test_res_scaled, CONFIG['window_size'])\n    \n    if len(X_train) == 0: return None\n\n    # D. Train GRU-TCN Model\n    print(f\"  [3/4] Training GRU-TCN ({len(X_train)} samples)...\")\n    model = build_tcn_hybrid_model(\n        input_shape=(CONFIG['window_size'], X_train.shape[2]),\n        output_dim=y_train.shape[1],\n        config=CONFIG\n    )\n    \n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=CONFIG['epochs'],\n        batch_size=CONFIG['batch_size'],\n        callbacks=[early_stop, reduce_lr],\n        verbose=0\n    )\n    print(f\"      > Final Val Loss: {history.history['val_loss'][-1]:.5f}\")\n    \n    # E. Final Prediction & Fusion\n    print(\"  [4/4] Forecasting & Fusion...\")\n    \n    # 1. Predict Scaled Residuals\n    pred_res_scaled = model.predict(X_test, verbose=0)\n    \n    # 2. Inverse Scale -> Real Residuals\n    pred_res = scaler.inverse_transform(pred_res_scaled)\n    \n    # 3. Align Baseline\n    baseline_aligned = test_var_baseline.iloc[CONFIG['window_size']:].values\n    y_true_aligned = test_raw.iloc[CONFIG['window_size']:].values\n    \n    # Truncate to matching lengths\n    min_len = min(len(baseline_aligned), len(pred_res))\n    baseline_aligned = baseline_aligned[:min_len]\n    y_true_aligned = y_true_aligned[:min_len]\n    pred_res = pred_res[:min_len]\n    \n    # 4. FUSION\n    final_forecast = baseline_aligned + pred_res\n    final_forecast = np.maximum(final_forecast, 0)\n    \n    # Metrics\n    mse = np.mean((y_true_aligned - final_forecast)**2, axis=0)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true_aligned - final_forecast), axis=0)\n    \n    # Plotting\n    plt.figure(figsize=(12, 4))\n    idx = 0 # Throughput\n    plt.plot(y_true_aligned[:, idx], label='Actual', color='black', alpha=0.6)\n    plt.plot(baseline_aligned[:, idx], label='VAR Only', color='orange', linestyle='--', alpha=0.7)\n    plt.plot(final_forecast[:, idx], label='VAR+GRU+TCN (Final)', color='red', linewidth=1.5)\n    plt.title(f\"Slice: {slice_name} | RMSE: {rmse[idx]:.2f}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return {\n        'slice': slice_name,\n        'rmse_throughput': rmse[0],\n        'mae_throughput': mae[0]\n    }\n\ndef main():\n    try:\n        # Auto-detect file\n        filepath = find_dataset_file()\n        CONFIG['dataset_path'] = filepath\n        \n        slices_data = load_and_engineer_features(filepath)\n        \n        results = []\n        for name, data in slices_data.items():\n            res = train_evaluate_slice(name, data)\n            if res: results.append(res)\n            \n        if results:\n            res_df = pd.DataFrame(results)\n            print(\"\\nFinal Results Summary (VAR + GRU + TCN):\")\n            print(res_df)\n    except Exception as e:\n        print(f\"Critical Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:46:12.706872Z","iopub.execute_input":"2026-01-05T16:46:12.707152Z","iopub.status.idle":"2026-01-05T16:49:41.386761Z","shell.execute_reply.started":"2026-01-05T16:46:12.707130Z","shell.execute_reply":"2026-01-05T16:49:41.386196Z"}},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n[SEARCH] Looking for dataset in /kaggle/input...\n[FOUND] Dataset located: /kaggle/input/feature-extracted/part-00000.csv\n\n[IO] Loading raw data: /kaggle/input/feature-extracted/part-00000.csv\n  > Processing Slice: MMTC\n  > Processing Slice: Naver\n  > Processing Slice: Youtube\n\n========================================\n PROCESSING: MMTC\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 10\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TCN (1379 samples)...\n      > Final Val Loss: 0.15890\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Naver\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TCN (2765 samples)...\n      > Final Val Loss: 0.21138\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Youtube\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training GRU-TCN (10483 samples)...\n      > Final Val Loss: 0.48476\n  [4/4] Forecasting & Fusion...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/292443332.py:341: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n  plt.figure(figsize=(12, 4))\n","output_type":"stream"},{"name":"stdout","text":"\nFinal Results Summary (VAR + GRU + TCN):\n     slice  rmse_throughput  mae_throughput\n0     MMTC      6063.935700     1112.649430\n1    Naver    897424.585549   653295.107602\n2  Youtube    934596.363458   761275.439305\n","output_type":"stream"}],"execution_count":22},{"id":"325aae0e-cd8c-41ab-b2ab-0d55e2ef556f","cell_type":"markdown","source":"## VRT - VAR-Residual-Transformer","metadata":{}},{"id":"3c90b3ed-8049-45b1-9d55-8caa7d3f8a8e","cell_type":"code","source":"\"\"\"\nVAR-Residual-Transformer (VRT) for 5G Slice Forecasting - KAGGLE VERSION\n------------------------------------------------------------------------\nArchitecture:\n1. Linear Stream: VAR (Vector Autoregression) for baseline trend.\n2. Non-Linear Stream: Transformer Encoder for residual (error) correction.\n3. Fusion: Forecast = VAR_Baseline + Transformer_Correction.\n\"\"\"\n\nimport os\nimport glob\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.api import VAR\n\n# ==========================================\n# 0. Kaggle Configuration & Setup\n# ==========================================\n# Suppress TF warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# GPU Memory Growth (Important for Kaggle Shared GPUs)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"[GPU] Acceleration Enabled\")\n    except RuntimeError as e:\n        print(e)\n\nCONFIG = {\n    # Path will be auto-detected\n    'dataset_path': None, \n    \n    # Data params\n    'train_split': 0.7,\n    'window_size': 24,       \n    'forecast_horizon': 1,   \n    \n    # VAR params\n    'var_max_lags': 15,\n    \n    # Transformer params\n    'head_size': 64,         \n    'num_heads': 4,          \n    'ff_dim': 128,           \n    'num_transformer_blocks': 3,\n    'dropout': 0.15,\n    'learning_rate': 1e-3,\n    'batch_size': 32,\n    'epochs': 150\n}\n\n# Reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# ==========================================\n# 1. Advanced Data Loading & Engineering\n# ==========================================\ndef find_dataset_file():\n    \"\"\"Auto-detects the dataset file in Kaggle input directory\"\"\"\n    search_path = '/kaggle/input'\n    print(f\"[SEARCH] Looking for dataset in {search_path}...\")\n    \n    # Recursively find any CSV or TXT file\n    for root, dirs, files in os.walk(search_path):\n        for file in files:\n            if file.endswith(\".csv\") or file.endswith(\".txt\") or file.endswith(\".dat\"):\n                full_path = os.path.join(root, file)\n                print(f\"[FOUND] Dataset located: {full_path}\")\n                return full_path\n    \n    raise FileNotFoundError(\"Could not find a dataset file in /kaggle/input\")\n\ndef load_and_engineer_features(filepath):\n    print(f\"\\n[IO] Loading raw data: {filepath}\")\n    \n    try:\n        # Try reading as standard CSV first (in case you uploaded a clean version)\n        try:\n            df = pd.read_csv(filepath)\n            # If it's the raw format, the first column usually contains '::'\n            if df.shape[1] < 2 or '::' in str(df.iloc[0,0]):\n                raise ValueError(\"Likely raw format\")\n        except:\n            # Fallback to the specific parsing logic provided\n            print(\"  > Detected raw format (parsing '::')...\")\n            df = pd.read_csv(filepath, sep='\\t', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n            \n            # Handle potential single column issues if sep isn't tab\n            if df.shape[1] == 1:\n                 df = pd.read_csv(filepath, sep=',', header=None, names=['slice_timestamp', 'bytes', 'packets'])\n\n            split_data = df['slice_timestamp'].str.split('::', expand=True)\n            df['slice_type'] = split_data[0]\n            df['timestamp'] = pd.to_numeric(split_data[1])\n            df['bytes'] = pd.to_numeric(df['bytes'], errors='coerce')\n            df['packets'] = pd.to_numeric(df['packets'], errors='coerce')\n\n        # Standardize column names if reading a clean CSV\n        if 'slice_label' in df.columns: df.rename(columns={'slice_label': 'slice_type'}, inplace=True)\n        \n        processed_slices = {}\n        \n        # Determine the grouping column\n        group_col = 'slice_type' if 'slice_type' in df.columns else 'slice_id'\n        if group_col not in df.columns:\n            # If no slice column, assume whole file is one slice\n            df['slice_type'] = 'Default_Slice'\n            group_col = 'slice_type'\n\n        for slice_id in df[group_col].unique():\n            print(f\"  > Processing Slice: {slice_id}\")\n            slice_df = df[df[group_col] == slice_id].sort_values('timestamp').copy()\n            \n            if len(slice_df) < 500: continue \n\n            # 2. Key Performance Indicators (KPIs)\n            # Map columns flexibly\n            if 'sum_bytes' in slice_df.columns: slice_df['throughput'] = slice_df['sum_bytes']\n            elif 'bytes' in slice_df.columns: slice_df['throughput'] = slice_df['bytes']\n            \n            if 'sum_packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['sum_packets']\n            elif 'packets' in slice_df.columns: slice_df['packet_rate'] = slice_df['packets']\n            \n            # 3. Velocity/Acceleration\n            slice_df['throughput_diff'] = slice_df['throughput'].diff()\n            slice_df['packet_diff'] = slice_df['packet_rate'].diff()\n            \n            # 4. Volatility\n            slice_df['volatility'] = slice_df['throughput'].rolling(5).std().fillna(0)\n            \n            # Features\n            cols = ['throughput', 'packet_rate', 'throughput_diff', 'packet_diff', 'volatility']\n            final_df = slice_df[cols].dropna()\n            \n            # Robustness\n            p99 = final_df.quantile(0.99)\n            final_df = final_df.clip(upper=p99, axis=1)\n            \n            processed_slices[slice_id] = final_df\n            \n        return processed_slices\n\n    except Exception as e:\n        print(f\"[ERROR] Loading failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {}\n\n# ==========================================\n# 2. Strict Leakage-Free VAR Baseline\n# ==========================================\ndef get_var_residuals(train_df, test_df, maxlags):\n    # Handle constant columns to prevent VAR crash\n    for col in train_df.columns:\n        if train_df[col].nunique() <= 1:\n            train_df[col] += np.random.normal(0, 1e-6, size=len(train_df))\n\n    # 1. Fit VAR on Train\n    model = VAR(train_df)\n    try:\n        lag_order_res = model.select_order(maxlags=maxlags)\n        lag_order = lag_order_res.aic\n        if lag_order < 1: lag_order = 10\n    except:\n        lag_order = 10\n        \n    var_results = model.fit(lag_order)\n    print(f\"  [VAR] Fitted with Lag Order: {lag_order}\")\n    \n    # 2. Train Residuals\n    train_pred = var_results.fittedvalues\n    train_actual = train_df.iloc[lag_order:]\n    train_residuals = train_actual - train_pred\n\n    # 3. Test Baseline (Rolling Forecast)\n    coefs = var_results.coefs\n    intercept = var_results.intercept\n    \n    history = pd.concat([train_df.iloc[-lag_order:], test_df])\n    history_values = history.values\n    \n    test_preds = []\n    \n    for i in range(lag_order, len(history_values)):\n        window = history_values[i-lag_order : i]\n        window_reversed = window[::-1]\n        \n        pred = intercept.copy()\n        for l in range(lag_order):\n            pred += np.dot(coefs[l], window_reversed[l])\n            \n        test_preds.append(pred)\n        \n    test_pred_df = pd.DataFrame(test_preds, index=test_df.index, columns=test_df.columns)\n    \n    # 4. Test Residuals\n    test_residuals = test_df - test_pred_df\n    \n    return train_residuals, test_residuals, test_pred_df\n\n# ==========================================\n# 3. Transformer Model Components\n# ==========================================\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = models.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout)\n        self.dropout2 = layers.Dropout(dropout)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\ndef build_transformer_model(input_shape, output_dim, config):\n    inputs = layers.Input(shape=input_shape)\n    \n    # 1. Learnable Positional Embedding\n    x = layers.Conv1D(filters=config['head_size'], kernel_size=1)(inputs)\n    \n    # 2. Transformer Blocks\n    for _ in range(config['num_transformer_blocks']):\n        x = TransformerBlock(\n            embed_dim=config['head_size'],\n            num_heads=config['num_heads'],\n            ff_dim=config['ff_dim'],\n            dropout=config['dropout']\n        )(x)\n        \n    # 3. Global Pooling\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(config['dropout'])(x)\n    \n    # 4. Output\n    outputs = layers.Dense(output_dim, activation='linear')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name=\"Residual_Transformer\")\n    \n    optimizer = optimizers.Adam(learning_rate=config['learning_rate'])\n    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n    \n    return model\n\n# ==========================================\n# 4. Helpers: Windowing\n# ==========================================\ndef create_windows(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i : i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# ==========================================\n# 5. Main Execution Pipeline\n# ==========================================\ndef train_evaluate_slice(slice_name, df):\n    print(f\"\\n{'='*40}\\n PROCESSING: {slice_name}\\n{'='*40}\")\n    \n    # A. Split Data\n    split_idx = int(len(df) * CONFIG['train_split'])\n    train_raw = df.iloc[:split_idx]\n    test_raw = df.iloc[split_idx:]\n    \n    if len(test_raw) < CONFIG['window_size'] + 50:\n        print(\"  [SKIP] Not enough data for testing.\")\n        return None\n\n    # B. The Linear Baseline (VAR)\n    print(\"  [1/4] Calculating VAR Residuals...\")\n    try:\n        train_res, test_res, test_var_baseline = get_var_residuals(\n            train_raw, test_raw, CONFIG['var_max_lags']\n        )\n    except Exception as e:\n        print(f\"  [VAR FAILED] {e}. Skipping slice.\")\n        return None\n    \n    # C. Preprocessing for Deep Learning\n    print(\"  [2/4] Scaling & Windowing...\")\n    scaler = StandardScaler()\n    train_res_scaled = scaler.fit_transform(train_res)\n    test_res_scaled = scaler.transform(test_res)\n    \n    X_train, y_train = create_windows(train_res_scaled, CONFIG['window_size'])\n    X_test, y_test = create_windows(test_res_scaled, CONFIG['window_size'])\n    \n    if len(X_train) == 0: return None\n\n    # D. Train Transformer\n    print(f\"  [3/4] Training Transformer ({len(X_train)} samples)...\")\n    model = build_transformer_model(\n        input_shape=(CONFIG['window_size'], X_train.shape[2]),\n        output_dim=y_train.shape[1],\n        config=CONFIG\n    )\n    \n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=CONFIG['epochs'],\n        batch_size=CONFIG['batch_size'],\n        callbacks=[early_stop, reduce_lr],\n        verbose=0\n    )\n    print(f\"      > Final Val Loss: {history.history['val_loss'][-1]:.5f}\")\n    \n    # E. Final Prediction & Fusion\n    print(\"  [4/4] Forecasting & Fusion...\")\n    \n    # 1. Predict Scaled Residuals\n    pred_res_scaled = model.predict(X_test, verbose=0)\n    \n    # 2. Inverse Scale -> Real Residuals\n    pred_res = scaler.inverse_transform(pred_res_scaled)\n    \n    # 3. Align Baseline\n    baseline_aligned = test_var_baseline.iloc[CONFIG['window_size']:].values\n    y_true_aligned = test_raw.iloc[CONFIG['window_size']:].values\n    \n    # Truncate to matching lengths\n    min_len = min(len(baseline_aligned), len(pred_res))\n    baseline_aligned = baseline_aligned[:min_len]\n    y_true_aligned = y_true_aligned[:min_len]\n    pred_res = pred_res[:min_len]\n    \n    # 4. FUSION\n    final_forecast = baseline_aligned + pred_res\n    final_forecast = np.maximum(final_forecast, 0)\n    \n    # Metrics\n    mse = np.mean((y_true_aligned - final_forecast)**2, axis=0)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(y_true_aligned - final_forecast), axis=0)\n    \n    # Plotting\n    plt.figure(figsize=(12, 4))\n    idx = 0 # Throughput\n    plt.plot(y_true_aligned[:, idx], label='Actual', color='black', alpha=0.6)\n    plt.plot(baseline_aligned[:, idx], label='VAR Only', color='orange', linestyle='--', alpha=0.7)\n    plt.plot(final_forecast[:, idx], label='VRT (Final)', color='blue', linewidth=1.5)\n    plt.title(f\"Slice: {slice_name} | RMSE: {rmse[idx]:.2f}\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    return {\n        'slice': slice_name,\n        'rmse_throughput': rmse[0],\n        'mae_throughput': mae[0]\n    }\n\ndef main():\n    try:\n        # Auto-detect file\n        filepath = find_dataset_file()\n        CONFIG['dataset_path'] = filepath\n        \n        slices_data = load_and_engineer_features(filepath)\n        \n        results = []\n        for name, data in slices_data.items():\n            res = train_evaluate_slice(name, data)\n            if res: results.append(res)\n            \n        if results:\n            res_df = pd.DataFrame(results)\n            print(\"\\nFinal Results Summary:\")\n            print(res_df)\n    except Exception as e:\n        print(f\"Critical Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:33:33.434334Z","iopub.execute_input":"2026-01-05T16:33:33.434692Z","iopub.status.idle":"2026-01-05T16:35:54.957854Z","shell.execute_reply.started":"2026-01-05T16:33:33.434668Z","shell.execute_reply":"2026-01-05T16:35:54.956879Z"}},"outputs":[{"name":"stdout","text":"Physical devices cannot be modified after being initialized\n[SEARCH] Looking for dataset in /kaggle/input...\n[FOUND] Dataset located: /kaggle/input/feature-extracted/part-00000.csv\n\n[IO] Loading raw data: /kaggle/input/feature-extracted/part-00000.csv\n  > Processing Slice: MMTC\n  > Processing Slice: Naver\n  > Processing Slice: Youtube\n\n========================================\n PROCESSING: MMTC\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 10\n  [2/4] Scaling & Windowing...\n  [3/4] Training Transformer (1379 samples)...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1767630821.962506     132 service.cc:152] XLA service 0x795f581c67c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1767630821.962557     132 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1767630821.962561     132 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1767630829.592317     132 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"      > Final Val Loss: 0.13456\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Naver\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training Transformer (2765 samples)...\n      > Final Val Loss: 0.26480\n  [4/4] Forecasting & Fusion...\n\n========================================\n PROCESSING: Youtube\n========================================\n  [1/4] Calculating VAR Residuals...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n  self._init_dates(dates, freq)\n","output_type":"stream"},{"name":"stdout","text":"  [VAR] Fitted with Lag Order: 15\n  [2/4] Scaling & Windowing...\n  [3/4] Training Transformer (10483 samples)...\n      > Final Val Loss: 0.48421\n  [4/4] Forecasting & Fusion...\n\nFinal Results Summary:\n     slice  rmse_throughput  mae_throughput\n0     MMTC      5935.675836     1170.855953\n1    Naver    901047.049856   657102.529425\n2  Youtube    933798.719192   762432.662301\n","output_type":"stream"}],"execution_count":19},{"id":"3b6e74db-f3c3-4d25-9e9d-993a7ab4d520","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}